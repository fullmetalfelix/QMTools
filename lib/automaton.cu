#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <time.h>
#include <assert.h>
#include <cuda.h>
#include <cuda_runtime.h>

#include "qmtools.h"
#include "automaton.h"
#include "molecule.h"
#include "vectors.h"

/* TODOs:

	the automaton works on a multifield grid
	field 0 = charge density
	field 1 = external potential (nuclear)
	field 2 = auxiliary field
	...
*/


__constant__ float c_NN[4000];


void automaton_set_NN(float *parameters, int size) {

	cudaError_t error;
	error = cudaMemcpyToSymbol(c_NN, parameters, sizeof(float)*size);
	assert(error == cudaSuccess);
}


/// Reset Q/A/B fields to 0. Leave the V field as it is.
void automaton_reset(Grid *g) {

	// reset charge field
	cudaMemset(g->d_qube, 0, sizeof(float)*g->npts);

	// reset A/B fields
	cudaMemset(&g->d_qube[g->npts*2], 0, sizeof(float)*g->npts*2);
}

/// Populate a field with the nuclear potential of the molecule.
/// Mapping: one thread for each voxel
__global__ void kernel_automaton_vnn(
	float 	dx, 		// grid step size
	float3 	grid0, 		// grid origin
	int 	natoms,
	int 	*Zs,		// atom types
	float3 	*coords, 	// atom coordinates in bohr
	float 	*field		// output field
	) {

	__shared__ float3 spos[100];
	__shared__ int sZs[100];

	uint sidx = threadIdx.x + threadIdx.y*B + threadIdx.z*B_2;
	if(sidx < natoms) {
		spos[sidx] = coords[sidx];
		sZs[sidx] = Zs[sidx];
	}
	__syncthreads();

	float vnn = 0;

	// compute voxel position
	float3 voxpos;
	voxpos.x = grid0.x + (blockIdx.x * B + threadIdx.x) * dx + 0.5f*dx;
	voxpos.y = grid0.y + (blockIdx.y * B + threadIdx.y) * dx + 0.5f*dx;
	voxpos.z = grid0.z + (blockIdx.z * B + threadIdx.z) * dx + 0.5f*dx;

	float r;

	// for each atom
	for(int i=0; i<natoms; i++) {

		r = (voxpos.x - spos[i].x) * (voxpos.x - spos[i].x);
		r+= (voxpos.y - spos[i].y) * (voxpos.y - spos[i].y);
		r+= (voxpos.z - spos[i].z) * (voxpos.z - spos[i].z);

		//r = sZs[i] * exp(-r);
		r = sZs[i] * rsqrt(r);
		vnn += r;
		__syncthreads();
	}


	sidx = (threadIdx.x + blockIdx.x*B);
	sidx+= (threadIdx.y + blockIdx.y*B) * gridDim.x * B;
	sidx+= (threadIdx.z + blockIdx.z*B) * gridDim.x * gridDim.y * B_2;

	field[sidx] = vnn;
}

/// Populate the second field of grid g with the nuclear potential
/// generated by molecule m.
void automaton_compute_vnn(Grid *g, Molecule *m) {

	cudaError_t cudaError;


	dim3 block(B,B,B);
	kernel_automaton_vnn<<<g->GPUblocks, block>>>(
		g->step,
		g->origin,
		m->natoms,
		m->d_types,
		m->d_coords,
		&g->d_qube[g->npts]
	); cudaDeviceSynchronize();

	cudaError = cudaGetLastError();
	if(cudaError != cudaSuccess)
		printf("kernel_automaton_vnn error: %s\n", cudaGetErrorString(cudaError));
	assert(cudaError == cudaSuccess);
	cudaError = cudaMemcpy(g->qube, g->d_qube, sizeof(float)*g->npts*2, cudaMemcpyDeviceToHost); assert(cudaError == cudaSuccess);
}



/// Populate a field with an initial seed for the electron density.
__global__ void kernel_automaton_qseed(
	float 	gridStep, 	// grid step size
	float3 	grid0,
	dim3 	n,
	int 	*Zs,
	float3 	*coords,
	float 	*qfield 	// output density field
	) {

	__shared__ float3 atom;
	__shared__ int Z;


	uint gidx = threadIdx.x + threadIdx.y * B + threadIdx.z * B_2;

	// get the atom coords from global memory
	if(gidx == 0) {
		atom = coords[blockIdx.x];
		atom.x -= grid0.x;
		atom.y -= grid0.y;
		atom.z -= grid0.z;
		Z = Zs[blockIdx.x];
	}
	__syncthreads();

	// compute in which block the atom is located
	// it might be at the edge between two blocks

	// this is the index in the unwrapped grid where the thread
	// should place its share of the nuclear charge
	uint3 i0;
	i0.x = (uint)floorf((atom.x) / gridStep) + threadIdx.x;
	i0.y = (uint)floorf((atom.y) / gridStep) + threadIdx.y;
	i0.z = (uint)floorf((atom.z) / gridStep) + threadIdx.z;

	/*if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0){
		printf("atom %i [%f %f %f] -> B[%i %i %i]T[%i %i %i]\n", blockIdx.x, atom.x, atom.y, atom.z,
			b0.x, b0.y, b0.z, t0.x, t0.y, t0.z);
	}*/

	// calculate the charge factor
	float p = (float)Z;
	p *= fabsf(atom.x - i0.x * gridStep) / gridStep;
	p *= fabsf(atom.y - i0.y * gridStep) / gridStep;
	p *= fabsf(atom.z - i0.z * gridStep) / gridStep;

	// save the electronic charge
	gidx = i0.x + i0.y*n.x + i0.z*n.x*n.y;
	qfield[gidx] = p;

	//if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0) {
		//qfield[gidx] = (float)Z;
	//}

}

void automaton_compute_qseed(Grid *g, Molecule *m) {

	cudaError_t cudaError;
	cudaError = cudaMemset(g->d_qube, 0, sizeof(float) * g->npts); assert(cudaError == cudaSuccess);

	// the grid has one block for each atom in the molecule
	// the block is a 2 cube
	dim3 block(2,2,2);
	
	// since the grid step is small (0.1ang) we can be quite sure
	// that two atoms will not be spread over the same grid points
	// because the molecules are optimised organics!
	kernel_automaton_qseed<<<m->natoms, block>>>(
		g->step,
		g->origin,
		g->shape,
		m->d_types,
		m->d_coords,
		g->d_qube
	); cudaDeviceSynchronize();
	cudaError = cudaGetLastError();
	if(cudaError != cudaSuccess)
		printf("kernel_automaton_qseed error: %s\n", cudaGetErrorString(cudaError));
	assert(cudaError == cudaSuccess);

	// this is for DEBUG
	cudaError = cudaMemcpy(g->qube, g->d_qube, sizeof(float)*g->npts, cudaMemcpyDeviceToHost); assert(cudaError == cudaSuccess);
}



// 64 threads for each grid point
// compute the evolved grid
__global__ void 
__launch_bounds__(512, 4)
kernel_automaton_evolve_brutal(
	float 	*qube, 	// input multifield qube
	uint 	npts, 	// number of grid points in a field
	uint 	nf, 	// number of fields
	float 	*qout 	// output multifield qube
	) {


	volatile __shared__ float inputs[8*B_3];
	float output[16];
	float final[3];
	final[0] = 0; final[1] = 0; final[2] = 0;
	float q1;
	volatile uint ridx, widx;
	//short3 dx;
	short3 r;

	for(ushort deltas=1; deltas < 64; deltas++) {

		if((deltas & 3) == 2) deltas++;
		if((deltas & 12) == 8) deltas+=4;
		if((deltas & 48) == 32) deltas+=16;

		r.x = threadIdx.x + blockIdx.x*B;
		r.y = threadIdx.y + blockIdx.y*B;
		r.z = threadIdx.z + blockIdx.z*B;
		ridx = r.x + r.y*gridDim.x*B + r.z*gridDim.x*gridDim.y*B_2;
		widx = (threadIdx.x + threadIdx.y*B + threadIdx.z*B_2)*8;
		for(ushort k=0; k<nf; k++)
			inputs[widx + k] = qube[ridx + k*npts];
		//__syncthreads();
		q1 = inputs[widx];


		// dx is in the first 2 bits of deltas
		short dx;

		// load the neigbour point

		dx = (deltas & 1)>0;
		if((deltas & 2)>0) dx *= -1;
		r.x = threadIdx.x + blockIdx.x*B + dx;
		if(r.x == -1) r.x = gridDim.x*B-1;
		if(r.x == gridDim.x*B) r.x = 0;

		dx = (deltas & 4)>0;
		if((deltas & 8)>0) dx *= -1;
		r.y = threadIdx.y + blockIdx.y*B + dx;
		if(r.y == -1) r.y = gridDim.y*B-1;
		if(r.y == gridDim.y*B) r.y = 0;

		dx = (deltas & 16)>0;
		if((deltas & 32)>0) dx *= -1;		
		r.z = threadIdx.z + blockIdx.z*B + dx;
		if(r.z == -1) r.z = gridDim.z*B-1;
		if(r.z == gridDim.z*B) r.z = 0;

		ridx = r.x + r.y*gridDim.x*B + r.z*gridDim.x*gridDim.y*B_2;
		widx += 4;
		for(ushort k=0; k<nf; k++)
			inputs[widx + k] = qube[ridx + k*npts];

		float q2 = inputs[widx];
		__syncthreads();
		
		widx -= 4;
		uint nnoffset = 0;


		// compute the first layer - 16 neurons, 8 inputs
		#pragma unroll
		for(ushort i=0; i<16; i++) {
			
			nnoffset = 9*i;

			// neuron output
			float f = 0;
			/*for(ushort j=0; j<8; j++) {
			//	f += c_NN[nnoffset+j] * inputs[widx + j];
				if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0 &&
					blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
					printf("NN1 input %i = %f - NN param %f\n", j, inputs[widx + j], c_NN[nnoffset+j]);
				}
			}*/
			f += c_NN[nnoffset+0] * inputs[widx + 0];
			f += c_NN[nnoffset+1] * inputs[widx + 1];
			f += c_NN[nnoffset+2] * inputs[widx + 2];
			f += c_NN[nnoffset+3] * inputs[widx + 3];
			f += c_NN[nnoffset+4] * inputs[widx + 4];
			f += c_NN[nnoffset+5] * inputs[widx + 5];
			f += c_NN[nnoffset+6] * inputs[widx + 6];
			f += c_NN[nnoffset+7] * inputs[widx + 7];
			f += c_NN[nnoffset+8];
			output[i] = tanhf(f);

			// now compute the neuron output using the inputs in opposite order
			f = 0;
			//for(ushort j=0; j<8; j++) {
			//	f += c_NN[nnoffset+j] * inputs[widx + ((4 + j)%8)];
			//}
			f += c_NN[nnoffset+0] * inputs[widx + 4];
			f += c_NN[nnoffset+1] * inputs[widx + 5];
			f += c_NN[nnoffset+2] * inputs[widx + 6];
			f += c_NN[nnoffset+3] * inputs[widx + 7];
			f += c_NN[nnoffset+4] * inputs[widx + 0];
			f += c_NN[nnoffset+5] * inputs[widx + 1];
			f += c_NN[nnoffset+6] * inputs[widx + 2];
			f += c_NN[nnoffset+7] * inputs[widx + 3];
			f += c_NN[nnoffset+8];
			output[i] -= tanhf(f);
		
			/*if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0 &&
				blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
				printf("NN1 output %i = %f \n", i, output[i]);
			}*/
		}

		// compute the second layer - 8 neurons, 16 inputs
		for(ushort i=0; i<8; i++) {
			
			nnoffset = 9*16 + 17*i; // 16+1 weights+bias

			// neuron output
			float f = 0;
			for(ushort j=0; j<16; j++) {
				f += c_NN[nnoffset+j] * output[j];

				/*if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0 &&
					blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
					printf("NN2 input %i = %f - NN param %f\n", j, output[j], c_NN[nnoffset+j]);
				}*/

			}
			inputs[widx+i] = tanhf(f+c_NN[nnoffset+16]) - tanhf(-f+c_NN[nnoffset+16]);
			/*if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0 &&
				blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
				printf("NN2 output %i = %f \n", i, inputs[widx+i]);
			}*/
		}

		// compute the last layer - 4 neurons, 8 inputs
		for(ushort i=0; i<4; i++) {
			
			nnoffset = 9*16 + 17*8 + 9*i; // 8+1 weights+bias

			// neuron output
			float f = 0;
			for(ushort j=0; j<8; j++) {
				f += c_NN[nnoffset+j] * inputs[widx + j];
				//f += c_NN[nnoffset+j] * output[j];
			}
			output[i] = tanhf(f+c_NN[nnoffset+9]) - tanhf(-f+c_NN[nnoffset+9]);
		}

		// now output[] has the transfer factors for the q V A B


		if(output[0] < 0) output[0] *= q1;
		else output[0] *= q2;

		float factor = (deltas & 1) + ((deltas & 4)>0) + ((deltas & 16)>0);
		factor = rsqrtf(factor);

		final[0] += output[0] * factor * DIFFQ;
		final[1] += output[2] * factor * DIFFA;
		final[2] += output[3] * factor * DIFFB;

		short3 r2 = r;
		r.x = threadIdx.x + blockIdx.x*B;
		r.y = threadIdx.y + blockIdx.y*B;
		r.z = threadIdx.z + blockIdx.z*B;

		/* DEBUG CODE
			//if(q1 != 0 || q2 != 0)
				//printf("transfer %i %i %i to %i %i %i = %e\n", r.x,r.y,r.z, r2.x,r2.y,r2.z, output[0] * factor * DIFFQ);
			// BUG! some outputs are missing?!
			// it appears some x,y,z trasfer to i,j,k but the opposite transfer is not happening

			// DEBUG
			//final[0] += q2*factor*DIFFQ;

			/*if(q1 != 0) {
				printf("R %i %i %i -- Q1 %f Q2 %f -- output %f \n",
					r.x, r.y, r.z, q1, q2, output[0] * factor * DIFFQ
				);
			}*/

			/*if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0 &&
				blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
				printf("block %i %i %i -- deltas: %i final[0]: %f -- factor %f \n", 
					blockIdx.x, blockIdx.y, blockIdx.z, deltas, final[0], factor
				);
			}* /
		*/ 
	}

	// we might want to have fields adsorbed/generated somehow?
	r.x = threadIdx.x + blockIdx.x*B;
	r.y = threadIdx.y + blockIdx.y*B;
	r.z = threadIdx.z + blockIdx.z*B;
	ridx = r.x + r.y*gridDim.x*B + r.z*gridDim.x*gridDim.y*B_2;
	widx = (threadIdx.x + threadIdx.y*B + threadIdx.z*B_2)*8;
	for(ushort k=0; k<nf; k++)
		inputs[widx + k] = qube[ridx + k*npts];
	q1 = inputs[widx];
	__syncthreads();

	// this computes a second neural net that handles fields generation based on local information
	for(ushort i=0; i<16; i++) {
		
		uint nnoffset = 316 + 5*i;

		// neuron output
		float f = 0;
		f += c_NN[nnoffset+0] * inputs[widx + 0];
		f += c_NN[nnoffset+1] * inputs[widx + 1];
		f += c_NN[nnoffset+2] * inputs[widx + 2];
		f += c_NN[nnoffset+3] * inputs[widx + 3];
		f += c_NN[nnoffset+4];
		output[i] = tanhf(f);
	}

	// compute the second layer - 8 neurons, 16 inputs
	for(ushort i=0; i<8; i++) {
		
		uint nnoffset = 316 + 5*16 + 17*i; // 16+1 weights+bias

		// neuron output
		float f = 0;
		for(ushort j=0; j<16; j++) {
			f += c_NN[nnoffset+j] * output[j];
		}
		inputs[widx+i] = tanhf(f+c_NN[nnoffset+16]);
	}

	// compute the last layer - 2 neurons, 8 inputs
	for(ushort i=0; i<2; i++) {
		
		uint nnoffset = 316 + 5*16 + 17*8 + 9*i; // 8+1 weights+bias

		// neuron output
		float f = 0;
		for(ushort j=0; j<8; j++) {
			f += c_NN[nnoffset+j] * inputs[widx + j];
		}
		output[i] = tanhf(f+c_NN[nnoffset+9]);
	}

	// make sure the adsorption coefficients are within 0-1
	output[0] = output[0] + final[1] * OneOverDIFFTOT;
	output[0] += qube[ridx + 2*npts] * (c_NN[550]);

	output[1] = output[1] + final[2] * OneOverDIFFTOT;
	output[1] += qube[ridx + 3*npts] * (c_NN[551]);


	/*if(qube[ridx] != 0) {
		printf("R %i %i %i -- Q: %f / %f -- %f\n",
			r.x, r.y, r.z, qout[ridx], q1, final[0]
		);
	}
	printf("R %i %i %i -- Q: %f\n", r.x, r.y, r.z, qout[ridx]);
	*/
	// save the final results
	// only Q, A and B channels are updated
	qout[ridx] = q1 + final[0] * OneOverDIFFTOT;
	qout[ridx + 1*npts] = qube[ridx + 1*npts]; // VNN is copied
	qout[ridx + 2*npts] = output[0];
	qout[ridx + 3*npts] = output[1];
}


__global__ void kernel_automaton_qrescale(float *qube, float factor) {

	float3 r;
	uint ridx;

	r.x = threadIdx.x + blockIdx.x*B;
	r.y = threadIdx.y + blockIdx.y*B;
	r.z = threadIdx.z + blockIdx.z*B;
	ridx = r.x + r.y*gridDim.x*B + r.z*gridDim.x*gridDim.y*B_2;

	qube[ridx] = qube[ridx] * factor;
}


__global__ void kernel_automaton_qtot(float *qube, float* qtot) {

	__shared__ float q[B_3];
	float3 r;
	uint ridx, widx;

	r.x = threadIdx.x + blockIdx.x*B;
	r.y = threadIdx.y + blockIdx.y*B;
	r.z = threadIdx.z + blockIdx.z*B;
	ridx = r.x + r.y*gridDim.x*B + r.z*gridDim.x*gridDim.y*B_2;
	widx = threadIdx.x + threadIdx.y*B + threadIdx.z*B_2;
	
	q[widx] = qube[ridx];
	__syncthreads();

	// do the partial sums: each thread adds the next
	float psum = q[widx];
	for(ushort stride=1; stride<B_3; stride*=2) {

		ushort idxnext = widx + stride;
		idxnext *= (idxnext < B_3);

		psum += q[idxnext];
		__syncthreads();

		q[widx] = psum;
		__syncthreads();
	}

	if(widx == 0)
		atomicAdd(qtot, psum);
}


__global__ void kernel_automaton_qcompare(float *qube, float *qref, float *qdiff) {

	__shared__ float q[B_3];

	float3 r;
	uint ridx, widx;

	r.x = threadIdx.x + blockIdx.x*B;
	r.y = threadIdx.y + blockIdx.y*B;
	r.z = threadIdx.z + blockIdx.z*B;
	ridx = r.x + r.y*gridDim.x*B + r.z*gridDim.x*gridDim.y*B_2;
	widx = threadIdx.x + threadIdx.y*B + threadIdx.z*B_2;
	
	q[widx] = fabsf(qube[ridx] - qref[ridx]);
	__syncthreads();

	// do the partial sums: each thread adds the next
	float psum = q[widx];
	for(ushort stride=1; stride<B_3; stride*=2) {

		ushort idxnext = widx + stride;
		idxnext *= (idxnext < B_3);

		psum += q[idxnext];
		__syncthreads();

		q[widx] = psum;
		__syncthreads();
	}
	
	//psum /= B_3; // average difference in this block

	if(widx == 0)
		atomicAdd(qdiff, psum);
}

float automaton_compare(Grid *g1, Grid *g2) {

	float *d_qdiff; cudaMalloc((void**)&d_qdiff, sizeof(float));
	float h_qdiff;

	cudaError_t cudaError;
	dim3 block(B,B,B);

	cudaMemset(d_qdiff, 0, sizeof(float));
	kernel_automaton_qcompare<<<g1->GPUblocks, block>>>(g1->d_qube, g2->d_qube, d_qdiff); cudaDeviceSynchronize();
	cudaError = cudaGetLastError();
	if(cudaError != cudaSuccess) printf("kernel_automaton_qcompare error: %s\n", cudaGetErrorString(cudaError));
	assert(cudaError == cudaSuccess);

	cudaMemcpy(&h_qdiff, d_qdiff, sizeof(float), cudaMemcpyDeviceToHost);
	cudaFree(d_qdiff);

	//h_qdiff /= g1->GPUblocks.x * g1->GPUblocks.y * g1->GPUblocks.z;
	return h_qdiff;
}



float automaton_compute_evolution(Grid *g, Molecule *m, int iterations, int copyback) {

	cudaError_t cudaError;
	dim3 block(B,B,B);

	float *qout; cudaMalloc((void**)&qout, sizeof(float)*g->npts*g->nfields);
	cudaMemset(qout, 0, sizeof(float)*g->npts*g->nfields);

	float *qtot; cudaMalloc((void**)&qtot, sizeof(float));
	float qtotCPU;


	for(int rep=0; rep<iterations; rep++) {

		//kernel_automaton_evolve<<<g->GPUblocks, block>>>(
		kernel_automaton_evolve_brutal<<<g->GPUblocks, block>>>(
			g->d_qube,
			g->npts,
			g->nfields,
			qout
		); cudaDeviceSynchronize();
		cudaError = cudaGetLastError();
		if(cudaError != cudaSuccess)
			printf("automaton_compute_evolution error: %s\n", cudaGetErrorString(cudaError));
		assert(cudaError == cudaSuccess);


		if((rep % 100) == 0) { // every so often check for electron leak
			
			// compute qtot
			cudaMemset(qtot, 0, sizeof(float));
			kernel_automaton_qtot<<<g->GPUblocks, block>>>(qout, qtot); cudaDeviceSynchronize();
			cudaError = cudaGetLastError();
			if(cudaError != cudaSuccess) printf("automaton_compute_qtot error: %s\n", cudaGetErrorString(cudaError));
			assert(cudaError == cudaSuccess);

			cudaMemcpy(&qtotCPU, qtot, sizeof(float), cudaMemcpyDeviceToHost);
			//printf("qtot: %f\n", qtotCPU);

			// rescale the total charge if needed
			if(abs(qtotCPU - m->qtot) > 0.001f) {

				float factor = m->qtot / qtotCPU;
				kernel_automaton_qrescale<<<g->GPUblocks, block>>>(qout, factor); cudaDeviceSynchronize();
				cudaError = cudaGetLastError();
				if(cudaError != cudaSuccess) printf("kernel_automaton_qrescale error: %s\n", cudaGetErrorString(cudaError));
				assert(cudaError == cudaSuccess);
			}
		}


		// switch the pointers
		float *tmp = g->d_qube;
		g->d_qube = qout;
		qout = tmp;
	}

	// often compare to the previous state to estimate q changes
	cudaMemset(qtot, 0, sizeof(float));
	kernel_automaton_qcompare<<<g->GPUblocks, block>>>(qout, g->d_qube, qtot); cudaDeviceSynchronize();
	cudaError = cudaGetLastError();
	if(cudaError != cudaSuccess) printf("kernel_automaton_qcompare error: %s\n", cudaGetErrorString(cudaError));
	assert(cudaError == cudaSuccess);

	cudaMemcpy(&qtotCPU, qtot, sizeof(float), cudaMemcpyDeviceToHost);
	//qtotCPU /= g->GPUblocks.x * g->GPUblocks.y * g->GPUblocks.z;
	//printf("qdiff: %f\n", qtotCPU);


	// copy results to cpu
	if(copyback > 0) {
		cudaError = cudaMemcpy(g->qube, g->d_qube, sizeof(float)*g->npts*g->nfields, cudaMemcpyDeviceToHost);
		assert(cudaError == cudaSuccess);
	}

	cudaFree(qout);
	return qtotCPU;
}

